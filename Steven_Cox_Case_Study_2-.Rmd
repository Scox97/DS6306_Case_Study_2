---
title: "Case Study 2 - DDSAnalytics "
author: "Steven Cox"
date: "4/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

# Required Libraries
library(caret)
library(class)
library(corrplot)
library(doParallel)
library(dplyr)
library(forcats)
library(FSelectorRcpp)
library(ggthemes)
library(ggplot2)
library(gridExtra)
library(janitor)
library(kableExtra)
library(knitr)
library(multcomp)
library(plotROC)
library(pROC)
library(ROCR)
library(plotROC)
library(tidyverse)

```
## Introduction
DDSAnalytics is a Fortune 100 company that has hired me to analyze employee attrition based on a companies research. My objective is to identify the top three factors that contribute to employee turnover within DDSAnalytics, and also to examine any job role specific trends that may exist in the data set.

To achieve this, a thorough analysis of the available employee data. This will likely involve cleaning and pre-processing the data, creating derived attributes/variables/features where necessary, and then applying appropriate statistical or machine learning techniques to identify the key factors contributing to turnover.

Once the top three factors have been identified, we can then examine whether there are any job role specific trends that may be contributing to these factors. This could involve looking at differences in turnover rates between job roles, or examining employee satisfaction levels within each role to identify areas that may require improvement. Overall, the goal is to provide DDSAnalytics with actionable insights that can help reduce employee turnover and improve talent management practices.
## Read the Data
```{r}
df <- read.csv("DataSet/CaseStudy2data.csv")

# Verify data frame
head(df)

#Show summary of data frame
summary(df)
str(df)

```
## Wrangling data frame to help with EDA.  
```{r}
#check for empty cells, NA's, and unique values
which(df == '')
missing_count <- df %>% sapply(function(x) sum(is.na(x)))
print(missing_count)
### No cells found to be empty or with missing values

###Show columns do not have any unique values:
df %>% 
  summarize_all(~ n_distinct(.)) %>%
  gather() %>%
  filter(value == 1) %>%
  pull(key) %>%
  paste(collapse = ", ") %>%
  cat("The following columns have only 1 unique value:", ., "\n")

###Remove one of a kind vectors:
df$EmployeeCount <- NULL
df$Over18 <- NULL
df$StandardHours <- NULL
```

### Get overall picture of the data
```{r}
# Start by analyzing the distribution of employess in Job Role and Department
ggplot(df, aes(x = Department, fill = Department)) +
  geom_bar(position = "stack") +
  labs(title = "Employee Count by Job Role and Department",
       x = "Job Role", y = "Count", fill = "Department") +
  geom_text(stat = "count", aes(label=..count..), position = position_stack(vjust = 0.5)) +
  coord_flip() + 
  theme_clean()

df %>% ggplot(., aes(x = JobRole, fill = Department)) +
  geom_bar(position = "stack") +
  labs(title = "Employee Count by Job Role and Department",
       x = "Job Role", y = "Count", fill = "Department") +
  geom_text(stat = "count", aes(label=..count..), position = position_stack(vjust = 0.5)) +
  coord_flip() + 
  theme_clean()

```
There is a good sample of observations present, though there is a strong presence of Research & Development over the other two groups.  This may come into play when we start modeling. 

## EDA - Attrition Distribution
```{r}
# Calculate the overall turnover rate
overall_attrition_rate <- round(mean(df$Attrition == "Yes")*100,1)
# Plot the attrition distribution
ggplot(df, aes(x = Attrition, fill = Attrition)) +
  geom_bar() +
  theme_bw() + 
  theme(legend.position = "none")+
  labs( title = "Attrition Count",
        x = "Attrition",
        y = "Count") +
  geom_text(aes(label = paste0(
    "Overall Rate: ",overall_attrition_rate, "%")),
    x = 2, y = sum(df$Attrition == "Yes") + 30, 
    size = 4, fontface = "bold", color = "black")

# Calculate attrition percentage per department
attr_summary <- aggregate(Attrition ~ Department, data = df, FUN = function(x) {
  mean(x == "Yes") * 100
})

# Create bar plot
p1 <- ggplot(attr_summary, 
             aes(x = Department, y = Attrition, fill = Department)) +
  geom_col() +
  geom_text(aes(label = round(Attrition, 1)), 
            position = position_dodge(width = 1), 
            vjust = +1, size = 3) +
  theme_bw() + 
  theme(legend.position = "none")+
  xlab("Department") +
  ylab("Attrition Percentage") +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5)) + 
  geom_hline(yintercept = overall_attrition_rate, 
             linetype = "dashed", 
             color = "red") +
  annotate("text", x = 2.5, y = overall_attrition_rate,vjust = +2, size = 3,
           label = paste0("Overall Attrition Rate: ", 
                          overall_attrition_rate, "%"))

# aggregate and create Attr_JobRole data frame
attr_summary <- aggregate(Attrition ~ Department + JobRole, data = df, FUN = function(x) c(Total = length(x), AttritionNo = sum(x == "No"), AttritionYes = sum(x == "Yes")))

# extract matrix of aggregated values
attrition_matrix <- attr_summary$Attrition

# assign new column names
colnames(attrition_matrix) <- c("Total", "AttritionNo", "AttritionYes")

# combine with grouping variables to form final data frame
attr_summary <- data.frame(Department = attr_summary[,1], JobRole = attr_summary[,2], attrition_matrix)

# add new columns
attr_summary$AttrNoRate <- attr_summary$AttritionNo / attr_summary$Total * 100
attr_summary$AttrYesRate <- attr_summary$AttritionYes / attr_summary$Total * 100

# create bar plot
p2 <- ggplot(attr_summary, aes(x = JobRole, y = AttrYesRate, fill = Department)) + 
  geom_col(position = "dodge") +
  theme_classic() + 
  theme(legend.position = "none") +
  labs(
    x = "Job Role", y = "Attrition Rate (%)",
    fill = "Department") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  # add turnover rate labels to the top of each bar
  geom_text(aes(label = round(AttritionYes)), 
            position = position_dodge(width = 1), 
            vjust = +1, size = 2.5) +
  # add overall turnover rate for reference
  geom_hline(yintercept = overall_attrition_rate, 
             linetype = "dashed", 
             color = "red") +
  annotate("text", x = 5, y = overall_attrition_rate + 2, vjust =-3, size = 3,
           label = paste0("Overall Attrition Rate: ", 
                          overall_attrition_rate, "%"))

grid.arrange(p1, p2, ncol = 2, 
             top = "Attrition Distribution by Department and Job Role",
             bottom = NULL,
             left = NULL,
             right = NULL)

```


## Digging a little deeper between Attrition and the other variables
```{r}
# Determine the pval between the other variables and Attrition using either a ttest if the varibale is normally distributed, or Wilcox test for those that are not. I will use the shaprio test to determine how the variable is distributed.  The null hypothesis of the Shapiro-Wilk test is that the data comes from a normal distribution, therefore if the pval is lower than the significance level (for our case 0.05), then we will reject the null and the variable will be considered skewed. If pval is above, then a ttest will be performed. For the variables that are not numeric, a chisq test will be performed. 

# create data frame to store p-values
p_values_df <- data.frame(name = character(), pval = numeric(), stringsAsFactors = FALSE)

# iterate over each variable in df
for (col in colnames(df)) {
  
  # if variable is numeric, perform Shapiro-Wilk test first
  if (is.numeric(df[[col]])) {
    shapiro_result <- shapiro.test(df[[col]])
    
    # if variable is normally distributed, perform t-test
    if (shapiro_result$p.value > 0.05) {
      ttest_result <- t.test(df[[col]] ~ df$Attrition)
      p_values_df <- rbind(p_values_df, 
                           data.frame(name = col, pval = ttest_result$p.value, 
                                      stringsAsFactors = FALSE))}
    
    # if variable is not normally distributed, perform Wilcoxon test
    else {
      wilcox_result <- wilcox.test(df[[col]] ~ df$Attrition)
      p_values_df <- rbind(p_values_df, 
                           data.frame(name = col, pval = wilcox_result$p.value, 
                                      stringsAsFactors = FALSE))}
  }
  
  # if variable is not numeric, perform chi-squared test
  else {
    chisq_result <- chisq.test(table(df[[col]], df$Attrition))
    p_values_df <- rbind(p_values_df, 
                         data.frame(name = col, pval = chisq_result$p.value, 
                                    stringsAsFactors = FALSE))}
}

## Loop through the df and determine if the pval indicates statistical significance
p_values_df$strong_relation <- ifelse(p_values_df$pval < 0.05, "Yes", "No")
## Sort the data frame accoriding to pval and inspect
p_values_df <- p_values_df[order(p_values_df$pval),]
head(p_values_df[-1,], n = 30)

## create significance_df by getting the names of all significant variables and creating a new data frame
significant_vars <- p_values_df %>% filter(pval < 0.05) %>% pull(name)
significant_vars
significant_df <- df[, c(significant_vars)]   

## of the variables that are significant, I want to check for multicollinearity by using a correlation matrix.  First I'll have to filter for numeric types
df_numeric <- Filter(is.numeric, significant_df) #Get only numeric values from df
cor_matrix <- cor(df_numeric)

## Gather variable pairs that have a correlation greater than 0.8, as that could be an indication of multicollinearity. 
cor_pairs <- function(cor_matrix, threshold = 0.8) {
  cor_matrix[lower.tri(cor_matrix)] <- NA
  pairs <- which(cor_matrix > threshold & cor_matrix < 1, arr.ind = TRUE)
  pairs <- data.frame(Var1 = rownames(cor_matrix)[pairs[,1]],
                      Var2 = colnames(cor_matrix)[pairs[,2]],
                      Correlation = cor_matrix[pairs])
  pairs <- pairs[!duplicated(paste(pmin(pairs$Var1, pairs$Var2), pmax(pairs$Var1, pairs$Var2))), ]
  return(pairs)
}
## Let's view the significant pairs with high correlation
significant_pairs <- cor_pairs(cor_matrix)
significant_pairs

# Visualize the correlation matrix
corrplot(cor_matrix,
         method = "color",
         type = "upper",
         order = "FPC",
         tl.col = "black",
         tl.srt = 90)

##Compare each of the two significant pairs to Attrition and determine if one should be removed

p_values_df[which(p_values_df$name == "MonthlyIncome"), 2]

p_values_df[which(p_values_df$name == "JobLevel"), 2]

# Monthly Income seems to have a lower p val and JobLevel has a closely resembling variable JobRoles.  Therefore JobLevel will be filtered out for modeling along with the other variables that did not show significance during our previous tests.
significant_df <- significant_df %>% dplyr::select(-JobLevel) 

# Job Satisfaction by Attrition
# perform t-test to calculate p-value
t_test <- t.test(JobSatisfaction ~ Attrition, data = df)
ggplot(df, aes(x = JobSatisfaction, fill = Attrition)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Job Satisfaction by Attrition", 
       y = "Count") + 
  scale_x_continuous(breaks = c(1, 2, 3, 4),
                     labels = c("Very Unsatisfied", 
                                "Unsatisfied", "Satisfied", "Very Satisfied")) +
  # add textbox with p-value
  annotate("text", x = 3.5, y = 0.05, hjust = 1, vjust = -0.5,
           label = paste("p = ", format.pval(t_test$p.value, digits = 2)))


# WorkLifeBalance by Attrition
# perform t-test to calculate p-value
t_test <- t.test(WorkLifeBalance ~ Attrition, data = df)
t_test
wilcox.test(WorkLifeBalance ~ Attrition, data = df, alternative = "two.sided")

ggplot(df, aes(x = WorkLifeBalance, fill = Attrition)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "WorkLife Balance by Attrition", 
       y = "Count") + 
  scale_x_continuous(breaks = c(1, 2, 3, 4),
                     labels = c("Very Unsatisfied", 
                                "Unsatisfied", "Satisfied", "Very Satisfied")) +
  # add textbox with p-value
  annotate("text", x = 3.2, y = 0.05, hjust = 1, vjust = -0.5,
           label = paste("p = ", format.pval(t_test$p.value, digits = 2)))

# WorkLifeBalance by Attrition
# perform t-test to calculate p-value
t_test <- t.test(WorkLifeBalance ~ Attrition, data = df)

ggplot(df, aes(x = WorkLifeBalance, fill = Attrition)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "WorkLife Balance by Attrition", 
       y = "Count") + 
  scale_x_continuous(breaks = c(1, 2, 3, 4),
                     labels = c("Very Unsatisfied", 
                                "Unsatisfied", "Satisfied", "Very Satisfied")) +
  # add textbox with p-value
  annotate("text", x = 3.2, y = 0.05, hjust = 1, vjust = -0.5,
           label = paste("p = ", format.pval(t_test$p.value, digits = 2)))

# Create the stacked bar chart
ggplot(df, aes(x = fct_infreq(JobRole), fill = BusinessTravel)) + 
  geom_bar() +
  labs(title = "Job Roles by Business Travel", x = "Job Role", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Salary vs Attrition
```{r}
ggplot(df, aes(x = Attrition, y = MonthlyIncome, fill = Attrition)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Salary Distribution by Attrition",
       x = "Attrition",
       y = "Monthly Income")

# Test whether salary is different between groups
t.test(
  df %>%
    filter(Attrition == "Yes") %>%
    pull(MonthlyIncome),
  df %>%
    filter(Attrition == "No") %>%
    pull(MonthlyIncome)
)

# Calculate quartiles
q1 <- quantile(df$MonthlyIncome, 0.25)
q2 <- quantile(df$MonthlyIncome, 0.5)
q3 <- quantile(df$MonthlyIncome, 0.75)

# Create factor levels based on quartiles
df$MonthlyIncomeCategory <- cut(df$MonthlyIncome, 
                                 breaks = c(-Inf, q1, q2, q3, Inf), 
                                 labels = c("Low", "Medium", "High", 
                                            "Very High"))
# Convert to factor
df$MonthlyIncomeCategory <- factor(df$MonthlyIncomeCategory)
```

#Preparing the data set for modeling

```{r}

## This code block prepares a data frame for modeling by converting character columns to factors, scaling numeric columns, and converting the target variable Attrition to a factor.
convert_char_to_factor <- function(df) {
  char_vars <- sapply(df, is.character)
  df[, char_vars] <- lapply(df[, char_vars], as.factor)
  return(df)
}

#Select the variables to scale
vars_to_scale <- c("MonthlyIncome", "Age", "DistanceFromHome", "YearsAtCompany",
                   "TotalWorkingYears", "YearsInCurrentRole", "JobInvolvement",
                   "YearsWithCurrManager", "JobSatisfaction", "WorkLifeBalance",
                    "EnvironmentSatisfaction", "StockOptionLevel")

df_model <- significant_df
# Scale the selected variables in the duplicate data frame
df_model[, vars_to_scale] <- scale(df_model[, vars_to_scale])

#Create df for modelling
df_model <- convert_char_to_factor(significant_df)

#convert Attrition to a factor
df_model$Attrition <- as.factor(df_model$Attrition)
```

# KNN Model
```{r}
# Train a KNN model using caret
# Set controls for 5-fold cross-validation with minority oversampling

train_ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5,
  classProbs = TRUE,
  savePredictions = TRUE,
  sampling = "up",
  summaryFunction = twoClassSummary
)

# Define tuning grid for k
tuningGrid <- expand.grid(
  k = seq(5, 50, by = 3)
)
# Train the model
set.seed(5)
knnModel <- train(Attrition ~ .,
                   data = df_model,
                   method = "knn",
                   trControl = train_ctrl,
                   tuneGrid = tuningGrid,
                   preProcess = c("center", "scale", "nzv"),
                   metric = "ROC",
                   tuneLength = 10,
                   na.action = na.omit
)
# Save the model object to disk
  saveRDS(knnModel, "Models/KNN_Model.rds")
  
# Sort and view best models
knnModel$results %>%
  mutate(metric = Sens + Spec) %>%
  arrange(desc(metric))
head(knnModel, n=5)

# Visualize the model
predictions <- predict(knnModel, newdata = df_model)
confusion_matrix <- confusionMatrix(predictions, as.factor(df_model$Attrition))
confusion_matrix

# Generate ROC curve data
selectedIndices <- knnModel$pred$k == knnModel$bestTune$k
knn_best <- knnModel$pred[selectedIndices, ]
knn_best$obs <- factor(knn_best$obs)
knn_roc_data <- roc(knn_best$obs, knn_best$Yes)
knn_auc <- knn_roc_data$auc

# Create ggroc plot
ggplot(knn_best, aes(d = obs, m = Yes)) +
  theme_bw() +
  geom_roc(n.cuts = 0, color = "orange") +
  labs(title = "ROC Curve for KNN", 
       x = "Specificity", y = "Sensitivity") +
  geom_text(x = 0.50, y = 0.3,  color = "orange", 
            label = paste("KNN AUC =", round(knn_auc, 2)))

```

# Naive Bayes Model
```{r}
## This code block trains a Naive Bayes classifier, evaluates its performance using multiple metrics, and creates visualizations of the results.

# Set controls for 5-fold cross-validation with minority oversampling
train_ctrl <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5,
    classProbs = TRUE,
    savePredictions = TRUE,
    sampling = "up",
    summaryFunction = twoClassSummary
)

# Define tuning grid for NB
tuningGrid <- expand.grid(usekernel = c(TRUE, FALSE),
                         laplace = seq(0,1, by = 0.2), 
                         adjust = seq(0.5, 1.9, by = 0.2))

# Train the model
set.seed(5)
nbModel <- train(Attrition ~ .,
                  data = df_model,
                  method = "naive_bayes",
                  trControl = train_ctrl,
                  tuneGrid = tuningGrid,
                  preProcess = c("center", "scale", "nzv"),
                  metric = "ROC",
                  na.action = na.omit)

# Save the model object to disk
saveRDS(nbModel, "Models/NB_Model.rds")
  
# Sort and view best models
nbModel$results %>%
    mutate(metric = Sens + Spec) %>%
    arrange(desc(metric))
head(nbModel, n=10)

# Evaluate the model using confusion matrix
predictions <- predict(nbModel, newdata = df_model)
confusion_matrix <- confusionMatrix(predictions, as.factor(df_model$Attrition))
confusion_matrix

# Evaluate the model using ROC and visualize
nb_results <- predict(nbModel, df_model, type = "prob")
nb_roc <- roc(df_model$Attrition, nb_results[, 2])
nb_auc <- round(auc(df_model$Attrition, nb_results[, 2]),4)

#create ROC plot
ggroc(nb_roc, size = 1, color = "red") +
  labs(title = "ROC Curve for Naive Bayes", x = "Specificity", y = "Sensitivity") +
  scale_color_manual(values = "red") +
  annotate("text", x = .25, y = .3, 
           label = paste0("NB AUC =", round(nb_auc,2))) +
  theme_clean()

```


# Logistic Modeling
```{r}
## This code trains a model using LogitBoost algorithm and evaluates its performance using ROC curve and confusion matrix. 

# Set controls for 5-fold cross-validation with minority oversampling
train_ctrl <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5,
    classProbs = TRUE,
    savePredictions = TRUE,
    sampling = "up",
    summaryFunction = twoClassSummary)


# Train the model
set.seed(5)
lbModel <- train(Attrition ~ .,
                  data = df_model,
                  method = "LogitBoost",
                  trControl = train_ctrl,
                  preProcess = c("center", "scale", "nzv"),
                  metric = "ROC",
                  na.action = na.omit)
# Save the model object to disk
saveRDS(lbModel, "Models/LogisticModel.rds")

summary(lbModel)
# Sort and view best models
lbModel$results %>%
    mutate(metric = Sens + Spec) %>%
    arrange(desc(metric))
head(lbModel, n=10)

# Evaluate the model using confusion matrix
log_predict <- predict(lbModel, newdata = df_model)
confusion_matrix <- confusionMatrix(log_predict, df_model$Attrition)
confusion_matrix

# Evaluate the model using ROC and visualize
log_predict <- predict(lbModel, df_model, type = "prob")
lr_roc <- roc(df_model$Attrition, log_predict[, 2])
lr_auc <- round(auc(df_model$Attrition, log_predict[, 2]),4)

#create ROC plot with minimal theme
ggroc(lr_roc, size = 1, color = "blue") +
  labs(title = "ROC Curve for LogitBoost Model",
       x = "Specificity", y = "Sensitivity") +
  theme_clean()+
  scale_color_manual(values = "blue") +
  annotate("text", x = .25, y = .3,  
                  label = paste("Logiboost AUC =", round(lr_auc,2)))

```

# Linear Regression Model
```{r}
library(caret)

# perform one-hot encoding
df$MonthlyIncome_log <- log(df$MonthlyIncome)
dummy_model <- dummyVars(MonthlyIncome ~ ., data = df)
df_train <- as.data.frame(predict(dummy_model, newdata = df))

# Set controls for 5-fold cross-validation
train_ctrl <- trainControl(
    method = "repeatedcv",
    number = 5,
    repeats = 5,
    savePredictions = TRUE
)

set.seed(5)
lrModel <- train(MonthlyIncome_log ~ .,
                 data = df_train,
                 trControl = train_ctrl,
                 method = "glmStepAIC",
                 direction = "both",
                 preProcess = c("center", "scale", "nzv", "BoxCox")
)
  
# Save the model object to disk
saveRDS(lrModel, "Models/LinReg.rds")

summary(lrModel)
defaultSummary(data.frame(pred = predict(lrModel, dfmodel_sub), obs = dfmodel_sub$MonthlyIncome_log))
varImp(lrModel, scale = TRUE)

# Plot the model
lrModel$finalModel %>% plot() 

```



#Predictions for Attrition - DDSAnalytics.
```{r}
# Create a DF of the Trained Data to use
trained_data <- lbModel

# Read in data to be predicted
data_to_predict <- read_csv("Test_Files/CaseStudy2CompSet No Attrition.csv")
# Verify data frame
head(data_to_predict)
summary(data_to_predict)
str(data_to_predict)

## Modify to_predict for modeling
to_predict <- convert_char_to_factor(data_to_predict)

# Select the variables to scale
vars_to_scale_list <- c("MonthlyIncome", "Age", "DistanceFromHome", "YearsAtCompany",
                   "TotalWorkingYears", "YearsInCurrentRole", "JobInvolvement",
                   "YearsWithCurrManager", "JobSatisfaction", "WorkLifeBalance",
                   "EnvironmentSatisfaction", "StockOptionLevel")

# Check if variable names to be scaled are in the list
# iterate over columns of data frame and scale if in vars_to_scale
for (col in names(to_predict)) {
  if (col %in% vars_to_scale_list) {
    to_predict[, col] <- scale(to_predict[, col])
  }
}

# Predict and Evaluate the model using confusion matrix
predictions <- predict(trained_data, newdata = to_predict)

# Save Predictions and write to file
predictions <- as.data.frame(predictions) 
results <- cbind(ID = to_predict$ID, Predictions = predictions)
write_csv(results, "Predictions/Case2PredictionsCox No Attrition.csv")

```

#Predictions for Salary - DDSAnalytics.
```{r}
# Create a DF of the Trained Data to use
trained_data <- lrModel

# Read in data to be predicted
data_to_predict <- read_csv("Test_Files/CaseStudy2CompSet No Salary.csv")
# Verify data frame
head(data_to_predict, n=5)
summary(data_to_predict)
str(data_to_predict)
###Remove one of a kind vectors:
data_to_predict$EmployeeCount <- NULL
data_to_predict$Over18 <- NULL
data_to_predict$StandardHours <- NULL
data_to_predict$MonthlyIncome <- as.numeric(data_to_predict$MonthlyIncome)
data_to_predict$MonthlyIncome_log <- data_to_predict$MonthlyIncome

# create a new data frame with the categorical variables encoded
data_to_predict_encoded <- predict(dummy_model, newdata = data_to_predict)

# make predictions on MonthlyIncome_log using the lrModel
predictions <- predict(lrModel, data_to_predict_encoded)

# add the predicted MonthlyIncome_log values
data_to_predict_encoded$MonthlyIncome_log_predicted <- predictions

# view the predicted values
head(data_to_predict_encoded$MonthlyIncome_log_predicted, n = 5)

predictions <- as.data.frame(predictions) 
results <- cbind(ID = data_to_predict$ID, MonthlyIncome = round(exp(predictions),2))
head(results, n=5)
write_csv(results, "Predictions/Case2PredictionsCox No Salary")

```

Steven Cox: sacox@mail.smu.edu

